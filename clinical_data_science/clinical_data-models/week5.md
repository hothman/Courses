## ETL steps

1. nderstand source and target table
2. Profile the source data table
3. Create ETL mapping 
4. Write the ETL code
5. Execute the ETL code
6. Make the data quality assessment
7. Generate the documentation 



### Step1: understand sourse/target data models

ERD diagram, technical documentation, user documentation

[OMOP documantation](https://github.com/OHDSI/CommonDataModel/wiki/PERSON)

[MiMIC tables](https://mimic.physionet.org/mimictables/)



### Step2: data profiling 

**Data profiling** is the process of examining the data available from an existing information source (e.g. a database or a file and collecting statistics or informative summaries about that data.

We use white rabbit to do the profiling 

### Step3: create the ETL mapping 

* Structural mapping (tables to tables) 
* Semantic mappings (values to values)

We use rabbit in a hat. 

### Step 4: Write the transformation code

We use rabbit in a hat. 

### Step5 and 6 Make the data quality assessment : run the code

### Step7 documentation 

* What was done 
* decision/assumptions made
* Unusual situations
* Transformation code

rabbit in a hat can assist in this step 








